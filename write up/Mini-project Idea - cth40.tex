\documentclass[11pt]{article}
\usepackage{a4wide,parskip,times}
\usepackage{multicol}
\PassOptionsToPackage{hyphens}{url}\usepackage[hidelinks]{hyperref}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage[margin=1.1in]{geometry}
\pagenumbering{gobble}
\titlespacing\subsubsection{0pt}{3pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}

\begin{document}

\centerline{\Large Automatic song suggestion using facial affect on mobile devices}
\vspace{1em}
\centerline{\Large \emph{Affective computing mini-project proposal}}
\vspace{2em}
\centerline{\large C. T. Hewitt (\emph{cth40}), Trinity Hall}
\vspace{2em}

% Mini-project description (~3 paragraphs)
\begin{multicols}{2}
\small

\subsubsection*{Overview}
I intend to develop a mobile application which uses the front facing camera to detect the user's affect from their facial expression. Based on the classification made from the video feed (or individual frames within) a list of songs will be recommended to match the current users mood.

\subsubsection*{Feature Extraction}
A number of options are available for feature extraction; OpenFace \cite{openface} can extract a number of facial landmarks from the input image data which can then be used for classification. Alternatively Affectiva's SDK \cite{affectiva} can extract a number of action unit based features.

\subsubsection*{Classification}
Classification will be carried out using an existing machine learning library. Weka \cite{weka} can be used on desktop to help determine the optimal feature set and technique, but is not generally suitable for mobile devices. Therefore, an alternative library will be used for the app itself, such as libsvm \cite{libsvm} or iOS's built in machine learning library, CoreML \cite{coreml}. Affectiva's SDK also includes some classification capabilities which could be used.

\subsubsection*{Training}
Training will be performed using a large, openly available dataset which provides appropriate annotations (ideally arousal and valence). There are a number of options available: \cite{data1}, \cite{data2}, \cite{data3}. Alternatively a pre-trained model may be employed if a suitable option is available.

\subsubsection*{Recommendation}
Song recommendations will be made using an external system, most likely the Spotify recommendations API \cite{spotify}.

\subsubsection*{Evaluation} 
The app will be evaluated via a user study. Participants will be asked to use the app while simulating a number of given facial expressions, then rate how well the app performs. Direct affect output (e.g. name of emotion or numeric ratings for valence/arousal etc.) will be given as well as song recommendations so that the affective aspect of the app and song recommendations can be evaluated independently. 

% List of tools/devices needed
\subsubsection*{Required Hardware}
\begin{itemize}[leftmargin=*]
\item{Laptop for development. I plan to use my own 2013 MacBook Pro, 2.8GHz i7, 16GB RAM.}
\item{Mobile phone with front facing camera. I plan to use my own iPhone 6s.}
\end{itemize}

% List of external software/libraries that will be utilised
\subsubsection*{External Software}
\begin{itemize}[leftmargin=*]
\item{Spotify recommendations API, takes as input various parameters such as \emph{danceability}, \emph{energy} and \emph{loudness} \cite{spotify}.}
\item{Libraries for feature extraction and classification of video data. This may include, but not be limited to: OpenFace \cite{openface}, Affectiva SDK \cite{affectiva}, Weka \cite{weka}, LibSVM \cite{libsvm}, iOS/MacOS SDK \cite{coreml}.}
\end{itemize}

%Papers:
%https://arxiv.org/pdf/1708.03985.pdf
%http://mohammadmahoor.com/affectnet/
{
\subsubsection*{References}
\def\section*#1{}
\begin{thebibliography}{9}
\scriptsize
\bibitem{openface}
OpenFace: an open source facial behavior analysis toolkit, Baltru\v{s}aitis et al. 2016.
\url{http://www.cl.cam.ac.uk/research/rainbow/projects/openface/}
\bibitem{affectiva}
Emotion SDK, Affectiva.\\
\url{https://www.affectiva.com/product/emotion-sdk/}
\bibitem{weka}
WEKA, E. Frank, M. Hall, and I. Witten, University of Waikato. 2016.\\
\url{https://www.cs.waikato.ac.nz/ml/weka/}
\bibitem{libsvm}
LibSVM, C. Chang and C. Lin.\\
\url{https://www.csie.ntu.edu.tw/~cjlin/libsvm/}
\bibitem{coreml}
Core ML, Apple Inc.\\
\url{https://developer.apple.com/documentation/coreml}
\bibitem{data1}
Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression Recognition in the Wild, S. Li and W. Deng. 2017.
\url{http://www.whdeng.cn/RAF/model1.html}
\bibitem{data2}
AR Face Database, A. Martinez, Ohio State University.
\url{http://www2.ece.ohio-state.edu/~aleix/ARdatabase.html}
\bibitem{data3}
DEAP: A Database for Emotion Analysis using Physiological Signals, Koelstra et al. 2012.
\url{http://www.eecs.qmul.ac.uk/mmv/datasets/deap/}
\bibitem{spotify}
Spotify API, Spotify Inc.
\url{https://developer.spotify.com/web-api/get-recommendations/}
\end{thebibliography}
}

\end{multicols}
\end{document}
